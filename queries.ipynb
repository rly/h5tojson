{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `python scrape_dandi.py` first to generate the JSON files in `dandi_json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function that runs a function on the first JSON file of each dandiset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "def run_on_first_json_per_dandiset(callable: callable):\n",
    "    \"\"\" Run a function on the first JSON file in each Dandiset in the \"dandi_json\" directory.\n",
    "\n",
    "    For each directory in the \"dandi_json\" directory, get the path of the first subject folder within the directory\n",
    "    For each subject folder, get the path of the first JSON file within the folder.\n",
    "    Call a function on the JSON file.\n",
    "    \"\"\"\n",
    "    # Get the list of directories in the \"dandi_json\" directory\n",
    "    directories = sorted(os.listdir(\"dandi_json\"))\n",
    "\n",
    "    # Iterate over each dandiset directory\n",
    "    for directory in directories:\n",
    "        # Get the path of the first folder within the directory\n",
    "        folder_path = os.path.join(\"dandi_json\", directory)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        # Get all JSON files no matter how many levels deep they are\n",
    "        # (Most dandisets have a single level of folders, but some have zero or two)\n",
    "        json_files = glob.glob(os.path.join(folder_path, \"**/*.json\"), recursive=True)\n",
    "\n",
    "        if len(json_files) == 0:\n",
    "            warnings.warn(f\"No JSON files found in {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        first_json_file = json_files[0]\n",
    "\n",
    "        # Call the function on the JSON file\n",
    "        callable(first_json_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the file name of the first JSON file in each Dandiset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_on_first_json_per_dandiset(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the listed species of each dandiset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_species_all = dict()\n",
    "def collect_subject_species(json_file: str):\n",
    "    \"\"\" Get the species of the subject in the JSON file. \"\"\"\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    species = data[\"file\"][\"groups\"][\"general\"][\"groups\"].get(\"subject\", {}).get(\"datasets\", {}).get(\"species\", {}).get(\"data\", None)\n",
    "    subject_species_all[json_file] = species\n",
    "\n",
    "run_on_first_json_per_dandiset(collect_subject_species)\n",
    "subject_species_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subject_species_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(subject_species_all.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What neurodata types are used by each dandiset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurodata_types_all = dict()\n",
    "def collect_all_neurodata_types(json_file: str):\n",
    "    \"\"\" Get all of the neurodata types used in each file. \"\"\"\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # for every group or dataset, if there is an attribute called \"neurodata_type\", add it to the list.\n",
    "    # the code below is less specific and just checks every dictionary key for \"neurodata_type\".\n",
    "    neurodata_types = []\n",
    "\n",
    "    def _recurse(data):\n",
    "        for key, value in data.items():\n",
    "            if key == \"neurodata_type\":\n",
    "                namespace = data[\"namespace\"]\n",
    "                neurodata_types.append(f\"{namespace}.{value}\")\n",
    "            elif isinstance(value, dict):\n",
    "                _recurse(value)\n",
    "    _recurse(data)\n",
    "\n",
    "    neurodata_types_all[json_file] = neurodata_types\n",
    "\n",
    "run_on_first_json_per_dandiset(collect_all_neurodata_types)\n",
    "neurodata_types_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "keys = [key for dandiset_nd_types in neurodata_types_all.values() for key in dandiset_nd_types]\n",
    "counts = Counter(keys)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"core.FeatureExtraction\" in counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What NWB schema, including extensions, are used by each dandiset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs_all = dict()\n",
    "def collect_all_specs(json_file: str):\n",
    "    \"\"\" Get all of the specs and their versions used in each file. \"\"\"\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    spec_versions = dict()  # map spec name to list of versions\n",
    "\n",
    "    # NOTE: some files have no \"specifications\" group\n",
    "    specs = data[\"file\"][\"groups\"].get(\"specifications\", {}).get(\"groups\", {})\n",
    "    for spec in specs:\n",
    "        spec_versions[spec] = list(specs[spec][\"groups\"].keys())  # the keys are the versions\n",
    "\n",
    "    specs_all[json_file] = spec_versions\n",
    "\n",
    "run_on_first_json_per_dandiset(collect_all_specs)\n",
    "specs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "keys = [key for spec_versions in specs_all.values() for key in spec_versions.keys()]\n",
    "Counter(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What dataset filters, such as compression, are used by each dandiset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_all = dict()\n",
    "def collect_all_filters(json_file: str):\n",
    "    \"\"\" Get all of the dataset filters used in each file. \"\"\"\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # for every dataset, if there is a key called \"filters\", add it to the list.\n",
    "    filters = []\n",
    "\n",
    "    def _recurse(data):\n",
    "        for key, value in data.items():\n",
    "            if key == \"datasets\":\n",
    "                for dataset_dict in value.values():\n",
    "                    filters.append(dataset_dict[\"filters\"])\n",
    "            elif isinstance(value, dict):\n",
    "                _recurse(value)\n",
    "    _recurse(data)\n",
    "\n",
    "    filters_all[json_file] = filters\n",
    "\n",
    "run_on_first_json_per_dandiset(collect_all_filters)\n",
    "filters_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which dandisets have a \"VoltageClampSeries/data\" dataset with an attribute \"IGORWaveNote\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igor_attribute_all = dict()\n",
    "def collect_all_with_igor_attr(json_file: str):\n",
    "    \"\"\" Get all of the dandisets that have a \"VoltageClamp/data\" dataset with an attribute \"IGORWaveNote\". \"\"\"\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    def _recurse(data):\n",
    "        for key, value in data.items():\n",
    "            if key == \"attributes\":\n",
    "                if \"neurodata_type\" in data[\"attributes\"]:\n",
    "                    # stop when one is found\n",
    "                    if data[\"attributes\"][\"neurodata_type\"] == \"VoltageClampSeries\":\n",
    "                        data_attrs = data[\"datasets\"][\"data\"][\"attributes\"]\n",
    "                        if \"IGORWaveNote\" in data_attrs:\n",
    "                            igor_attribute_all[json_file] = \"VoltageClampSeries\"\n",
    "                            break\n",
    "                    if data[\"attributes\"][\"neurodata_type\"] == \"CurrentClampSeries\":\n",
    "                        data_attrs = data[\"datasets\"][\"data\"][\"attributes\"]\n",
    "                        if \"IGORWaveNote\" in data_attrs:\n",
    "                            igor_attribute_all[json_file] = \"CurrentClampSeries\"\n",
    "                            break\n",
    "            elif isinstance(value, dict):\n",
    "                _recurse(value)\n",
    "    _recurse(data)\n",
    "\n",
    "run_on_first_json_per_dandiset(collect_all_with_igor_attr)\n",
    "\n",
    "# sort the keys\n",
    "keys = list(igor_attribute_all.keys())\n",
    "keys.sort()\n",
    "igor_attribute_all_sorted = {i: igor_attribute_all[i] for i in keys}\n",
    "# igor_attribute_all_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the shape of each ElectricalSeries in each dandiset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Path query\n",
    "# $..groups[?(@.attributes?.neurodata_type == \"ElectricalSeries\")].datasets.data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arrays",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
