{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "from linked_arrays import H5ToJson\n",
    "\n",
    "data = np.arange(10000).reshape(1000, 10)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_path = \"test.h5\"\n",
    "with h5py.File(hdf5_file_path, \"w\") as f:\n",
    "    f.create_dataset(\"data\", data=data, chunks=(600, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"test.json\"\n",
    "translator = H5ToJson(hdf5_file_path, json_file_path)\n",
    "translator.translate()\n",
    "\n",
    "with open(json_file_path) as f:\n",
    "    json_dict = json.load(f)\n",
    "\n",
    "print(json.dumps(json_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_json = {\n",
    "    \"version\": 1,\n",
    "    \"refs\": {\n",
    "        # \".zgroup\": \"{\\n    \\\"zarr_format\\\": 2\\n}\",\n",
    "        \"data/.zattrs\": \"{}\",\n",
    "        \"data/.zarray\": \"{\\\"chunks\\\":[600,8],\\\"compressor\\\":null,\\\"dtype\\\":\\\"<i8\\\",\\\"fill_value\\\":null,\\\"filters\\\":null,\\\"order\\\":\\\"C\\\",\\\"shape\\\":[1000,10],\\\"zarr_format\\\":2}\",\n",
    "        \"data/0.0\": [hdf5_file_path, 4016, 38400],\n",
    "        \"data/0.1\": [hdf5_file_path, 42416, 38400],\n",
    "        \"data/1.0\": [hdf5_file_path, 80816, 38400],\n",
    "        \"data/1.1\": [hdf5_file_path, 119216, 38400]\n",
    "    }\n",
    "}\n",
    "# we can also do fancier, space-efficient things with v1 spec https://fsspec.github.io/kerchunk/spec.html\n",
    "\n",
    "# see also referencefilesystem decoding scheme\n",
    "# https://github.com/fsspec/filesystem_spec/blob/master/fsspec/implementations/reference.py#L899\n",
    "# https://github.com/fsspec/filesystem_spec/blob/master/fsspec/implementations/reference.py#L692\n",
    "\n",
    "# fsspec unpacks these refs into a directory store as if they were files\n",
    "# https://github.com/fsspec/filesystem_spec/blob/master/fsspec/implementations/reference.py#L979\n",
    "\n",
    "# this may be the code that handles getting a requested data chunk\n",
    "# https://github.com/fsspec/filesystem_spec/blob/master/fsspec/implementations/reference.py#L744C9-L744C17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "mapper = fsspec.get_mapper(\n",
    "    'reference://',\n",
    "    fo=mock_json,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "z = zarr.open(mapper)\n",
    "z.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = z[\"data\"]\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_json = {  # reference file system format version 0\n",
    "    # \".zgroup\": \"{\\n    \\\"zarr_format\\\": 2\\n}\",  # <-- this does not seem to be necessary but is probably good to have\n",
    "    # \"data/.zattrs\": \"{}\",\n",
    "    \"data/.zarray\": \"{\\\"chunks\\\":[600,8],\\\"compressor\\\":null,\\\"dtype\\\":\\\"<i8\\\",\\\"fill_value\\\":null,\\\"filters\\\":null,\\\"order\\\":\\\"C\\\",\\\"shape\\\":[1000,10],\\\"zarr_format\\\":2}\",\n",
    "    \"data/0.0\": [hdf5_file_path, 4016, 38400],\n",
    "    \"data/0.1\": [hdf5_file_path, 42416, 38400],\n",
    "    \"data/1.0\": [hdf5_file_path, 80816, 38400],\n",
    "    \"data/1.1\": [hdf5_file_path, 119216, 38400]\n",
    "}\n",
    "\n",
    "import fsspec\n",
    "mapper = fsspec.get_mapper(\n",
    "    'reference://',\n",
    "    fo=mock_json,\n",
    ")\n",
    "\n",
    "import zarr\n",
    "z = zarr.open(mapper)\n",
    "print(z.info)\n",
    "\n",
    "arr = z[\"data\"]\n",
    "print(arr.info)\n",
    "arr[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fsspec.implementations.reference import ReferenceFileSystem\n",
    "fs = ReferenceFileSystem(fo=mock_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.cat(\"data/0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "zarray_props = json.loads(fs.cat(\"data/.zarray\"))\n",
    "zarray_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basically try to reverse engineer how zarr makes arrays given the chunk info\n",
    "# the simple case is easy but when filters are involved it gets more complicated\n",
    "data = np.empty(shape=zarray_props[\"shape\"], dtype=np.dtype(zarray_props[\"dtype\"]))\n",
    "data[0:600,0:8] = np.frombuffer(fs.cat_file(\"data/0.0\"), dtype=np.int64).reshape(600, 8)\n",
    "# data[0:600,8:10] = np.frombuffer(fs.cat_file(\"data/0.1\"), dtype=np.int64).reshape(600, 8)[0:600, 0:2]\n",
    "data[0:600,8:16] = np.frombuffer(fs.cat_file(\"data/0.1\"), dtype=np.int64).reshape(600, 8)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = zarr.array([1, 2, 3])\n",
    "z[:]\n",
    "z.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(\"{1: 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "from linked_arrays import H5ToJson\n",
    "\n",
    "hdf5_file_path = \"test_str.h5\"\n",
    "with h5py.File(hdf5_file_path, \"w\") as f:\n",
    "    f.create_dataset(\"data\", data=[\"a\", \"b\", \"c\"], dtype=h5py.string_dtype(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"test.json\"\n",
    "chunk_refs_file_path = \"test_chunks.json\"\n",
    "translator = H5ToJson(hdf5_file_path, json_file_path, chunk_refs_file_path)\n",
    "translator.translate()\n",
    "\n",
    "with open(json_file_path) as f:\n",
    "    json_dict = json.load(f)\n",
    "\n",
    "print(json.dumps(json_dict, indent=4))\n",
    "\n",
    "with open(chunk_refs_file_path) as f:\n",
    "    chunk_refs = json.load(f)\n",
    "\n",
    "print(json.dumps(chunk_refs, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how would zarr know that these are variable length strings? it only see that dtype=object..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_json = {  # reference file system format version 0\n",
    "    # \".zgroup\": \"{\\n    \\\"zarr_format\\\": 2\\n}\",  # <-- this does not seem to be necessary but is probably good to have\n",
    "    # \"data/.zattrs\": \"{}\",\n",
    "    \"data/.zarray\": \"{\\\"chunks\\\":[3],\\\"compressor\\\":null,\\\"dtype\\\":\\\"object\\\",\\\"fill_value\\\":null,\\\"filters\\\":null,\\\"order\\\":\\\"C\\\",\\\"shape\\\":[3],\\\"zarr_format\\\":2}\",\n",
    "    \"data/0\": [hdf5_file_path, 2048, 48],\n",
    "}\n",
    "\n",
    "import fsspec\n",
    "mapper = fsspec.get_mapper(\n",
    "    'reference://',\n",
    "    fo=mock_json,\n",
    ")\n",
    "\n",
    "import zarr\n",
    "z = zarr.open(mapper)\n",
    "print(z.info)\n",
    "\n",
    "arr = z[\"data\"]\n",
    "print(arr.info)\n",
    "arr[:]\n",
    "\n",
    "# the first filter must be an object codec..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basically reverse engineer the zarr chunk encoding scheme\n",
    "# the simple case is easy but when filters are involved it gets more complicated\n",
    "from fsspec.implementations.reference import ReferenceFileSystem\n",
    "fs = ReferenceFileSystem(fo=mock_json)\n",
    "zarray_props = json.loads(fs.cat(\"data/.zarray\"))\n",
    "print(fs.cat_file(\"data/0\"))\n",
    "print(zarray_props)\n",
    "\n",
    "data = np.empty(shape=zarray_props[\"shape\"], dtype=np.dtype(zarray_props[\"dtype\"]))\n",
    "# data[:] = np.frombuffer(fs.cat_file(\"data/0\"), dtype=str)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fs.cat_file(\"data/0\")\n",
    "print(data)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.frombuffer(fs.cat_file(\"data/0\"), dtype='S1')\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an in-memory HDF5 file using h5py\n",
    "with h5py.File('in_memory_data.h5', 'w', driver='core', backing_store=False) as h5f:\n",
    "    # Create a dataset from the buffer\n",
    "    dataset = h5f.create_dataset('variable_strings', data=np.void(data))\n",
    "    print(dataset)\n",
    "\n",
    "    # Read the dataset into a NumPy array\n",
    "    data = dataset[()]\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 file size: 72.628704 MB\n",
      "JSON file size: 117.218 KB\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from linked_arrays import H5ToJson\n",
    "\n",
    "hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000004/sub-P11HMH/sub-P11HMH_ses-20061101_ecephys+image.nwb\"\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000015/sub-an041/sub-an041_ses-20140821_obj-17pzgym.nwb\"\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000017/sub-Cori/sub-Cori_ses-20161214T120000.nwb\"\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000021/sub-699733573/sub-699733573_ses-715093703_probe-810755797_ecephys.nwb\"\n",
    "# # below takes a long time because there is a dataset with 393k chunks (est. time 50 min). set chunk_refs_file_path = None\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000028/sub-MEAREC-250neuron-Neuropixels/sub-MEAREC-250neuron-Neuropixels_ses-20200727T094620_ecephys.nwb\"\n",
    "# # below takes a long time because there is a dataset with 253k chunks (est. time 18 min). set chunk_refs_file_path = None\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000028/sub-mouse412804/sub-mouse412804_ses-20200803T115732_ecephys.nwb\"\n",
    "# # below takes a long time because there is a dataset with 65k chunks (est. time 1 min) and one with 262k chunks (est. time 15 min). set chunk_refs_file_path = None\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000037/sub-408021/sub-408021_ses-758519303_behavior+image+ophys.nwb\"\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000048/sub-222549/sub-fly01_ophys.nwb\"\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000049/sub-661968859/sub-661968859_ses-681698752_behavior+ophys.nwb\"\n",
    "# # below takes a long time because there is a dataset with 196k chunks (est. time 11 min). set chunk_refs_file_path = None\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000053/sub-npI1/sub-npI1_ses-20190413_behavior+ecephys.nwb\"\n",
    "# # below takes a long time because there is a dataset with 1.2M chunks (est. time 2.5 hours). set chunk_refs_file_path = None\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000054/sub-F1/sub-F1_ses-20190407T210000_behavior+ophys.nwb\"\n",
    "# # this file ends up pretty big because the voxel mask is a large struct array (int, int, int, float)...\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000167/sub-163/sub-163_ses-20200212T160655_ophys.nwb\"\n",
    "# # setting chunk_refs_file_path = None from here on out...\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000223/sub-2282/sub-2282_ses-20190914T145458_ecephys+ophys.nwb\"\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000231/sub-219CR/sub-219CR_ses-20190403T123013_behavior+image.nwb\"\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000296/sub-10002342988018666858/sub-10002342988018666858_ses-20170911T135306_ophys.nwb\"\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000402/sub-17797/sub-17797_ses-4-scan-10_behavior+image+ophys.nwb\"\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000409/sub-CSHL047/sub-CSHL047_ses-b52182e7-39f6-4914-9717-136db589706e_behavior+ecephys+image.nwb\"\n",
    "# hdf5_file_path = \"/Users/rly/Documents/NWB_Data/dandisets/000575/sub-02/sub-02_ses-20171011T152100_behavior+ecephys.nwb\"\n",
    "json_file_path = \"test.json\"\n",
    "chunk_refs_file_path = None  #\"test_chunks.json\"\n",
    "translator = H5ToJson(hdf5_file_path, json_file_path, chunk_refs_file_path)\n",
    "translator.translate()\n",
    "\n",
    "hdf5_file_size = os.path.getsize(hdf5_file_path)  # in bytes\n",
    "if hdf5_file_size > 1e9:\n",
    "    print(f\"HDF5 file size: {hdf5_file_size / 1e9} GB\")\n",
    "elif hdf5_file_size > 1e6:\n",
    "    print(f\"HDF5 file size: {hdf5_file_size / 1e6} MB\")\n",
    "else:\n",
    "    print(f\"HDF5 file size: {hdf5_file_size / 1000} KB\")\n",
    "\n",
    "json_file_size = os.path.getsize(json_file_path)  # in bytes\n",
    "if json_file_size > 1e6:\n",
    "    print(f\"JSON file size: {json_file_size / 1e6} MB\")\n",
    "else:\n",
    "    print(f\"JSON file size: {json_file_size / 1000} KB\")\n",
    "\n",
    "# with open(json_file_path) as f:\n",
    "#     json_dict = json.load(f)\n",
    "# print(json.dumps(json_dict, indent=4))\n",
    "# with open(chunk_refs_file_path) as f:\n",
    "#     chunk_refs = json.load(f)\n",
    "# print(json.dumps(chunk_refs, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = json_dict[\"refs\"][\"/\"][\"groups\"][\"general\"][\"groups\"][\"extracellular_ephys\"][\"groups\"][\"electrodes\"][\"datasets\"][\"imp\"][\"data\"][0]\n",
    "print(val, type(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "np.frombuffer(base64.b64decode(\"AQAAAAAAAAACAAAAAAAAAAMAAAAAAAAA\"), dtype=\"int64\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arrays",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
